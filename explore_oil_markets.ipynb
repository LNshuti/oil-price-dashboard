{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6ed01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "# Set plotting style\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the text file\n",
    "data = pd.read_csv('data/raw/PET/PET.txt', sep='\\t', header=None, names=['json_str'])\n",
    "\n",
    "# Function to parse JSON strings\n",
    "def parse_json_str(json_str):\n",
    "    return json.loads(json_str)\n",
    "\n",
    "# Apply the function to parse the JSON strings\n",
    "parsed_data = data['json_str'].apply(parse_json_str)\n",
    "\n",
    "# Create a DataFrame from the parsed JSON data\n",
    "df = pd.json_normalize(parsed_data)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9482765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the 'data' column to separate rows for each date-value pair\n",
    "df = df.explode('data')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select distinct series_id, name, units\n",
    "df_series = df[['series_id', 'name', 'units', 'unitsshort']].drop_duplicates()\n",
    "df_series.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c57b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on columns: 'series_id', 'units'\n",
    "df_series_nonas = df_series[(df_series['series_id'].notna()) & (df_series['units'].notna())]\n",
    "df_series_nonas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efc1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range_all = df.groupby('name')['end'].agg(['min', 'max'])\n",
    "date_range_all_nonas = date_range_all[(date_range_all['min'].notna()) & (date_range_all['max'].notna())]\n",
    "date_range_all_nonas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a count of the number of records by units \n",
    "df_series['units'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df to only include name containing 'Louisiana' and units in Dollars per Gallon\n",
    "df_louisiana = df[df['name'].str.contains('Louisiana Total') & df['units'].str.contains('Dollars per Gallon')]\n",
    "df_louisiana.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17028f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'data' is NaN or not a list\n",
    "df_louisiana = df_louisiana.dropna(subset=['data'])\n",
    "df_louisiana = df_louisiana[df_louisiana['data'].apply(lambda x: isinstance(x, list) and len(x) == 2)]\n",
    "df_louisiana.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba622fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 'data' column into 'date' and 'value'\n",
    "df_louisiana[['date', 'value']] = pd.DataFrame(df_louisiana['data'].tolist(), index=df_louisiana.index)\n",
    "df_louisiana.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5718761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' to datetime, coercing errors to NaT\n",
    "df_louisiana['date'] = pd.to_datetime(df_louisiana['date'], format='%Y%m%d', errors='coerce')\n",
    "# Convert 'value' to a numeric type, coercing errors to NaN\n",
    "df_louisiana['value'] = pd.to_numeric(df_louisiana['value'], errors='coerce')\n",
    "df_louisiana.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bcf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_louisiana.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns and preprocess the data\n",
    "df_louisiana['Date'] = pd.to_datetime(df_louisiana['date'])\n",
    "df_louisiana['Price'] = df_louisiana['value']\n",
    "df_louisiana = df_louisiana[['Date', 'Price', 'unitsshort', 'series_id', 'name', 'last_updated']].sort_values(by='Date').reset_index(drop=True)\n",
    "df_louisiana.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range_la = df_louisiana.groupby(['name', 'series_id'])['Date'].agg(['min', 'max'])\n",
    "date_range_la.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the table when both min and max are not equal to NaT \n",
    "date_range_la_nonas = date_range_la[(date_range_la['min'].notna()) & (date_range_la['max'].notna())]\n",
    "date_range_la_nonas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13752fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "def load_gas_price_data(filepath):\n",
    "    \"\"\"\n",
    "    Load and prepare the dataset from a raw text file containing JSON strings.\n",
    "    \n",
    "    Parameters:\n",
    "    - filepath: Path to the .txt file containing the raw data.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame with the data extracted from JSON strings, focusing on 'date' and 'value' columns.\n",
    "    \"\"\"\n",
    "    # Load the data from the text file\n",
    "    data = pd.read_csv(filepath, sep='\\t', header=None, names=['json_str'])\n",
    "    \n",
    "    # Function to parse JSON strings\n",
    "    def parse_json_str(json_str):\n",
    "        return json.loads(json_str)\n",
    "    \n",
    "    # Apply the function to parse the JSON strings\n",
    "    parsed_data = data['json_str'].apply(parse_json_str)\n",
    "    \n",
    "    # Create a DataFrame from the parsed JSON data\n",
    "    df = pd.json_normalize(parsed_data)\n",
    "    \n",
    "    # Assuming 'date' and 'value' are keys in the JSON data, convert 'date' to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    \n",
    "    # Keep only necessary columns and drop NA values\n",
    "    #df = df[['date', 'value']].dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Prepare data for long format and additional transformations\n",
    "def prepare_data(df, series_id=\"PET.EMA_EPM0_PBS_SLA_DPG.M\"):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for analysis by performing several transformations.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to be transformed.\n",
    "    - series_id: The series ID to filter the DataFrame by. Default is \"PET.EMA_EPM0_PBS_SLA_DPG.M\".\n",
    "    \n",
    "    Returns:\n",
    "    - Transformed DataFrame.\n",
    "    \"\"\"\n",
    "    # Filter based on series_id and non-NA 'units' column\n",
    "    df = df[df['series_id'] == series_id]\n",
    "    df = df.dropna(subset=['units'])\n",
    "    \n",
    "    # Explode the 'data' column to separate rows for each date-value pair\n",
    "    df = df.explode('data')\n",
    "    \n",
    "    # Drop rows where 'data' is NaN or not a list\n",
    "    df = df.dropna(subset=['data'])\n",
    "    df = df[df['data'].apply(lambda x: isinstance(x, list) and len(x) == 2)]\n",
    "    \n",
    "    # Split 'data' column into 'date' and 'value'\n",
    "    df[['date', 'value']] = pd.DataFrame(df['data'].tolist(), index=df.index)\n",
    "    \n",
    "    # Remove the 'data' column\n",
    "    df = df.drop(columns=['data'])\n",
    "    \n",
    "    # Convert 'date' to datetime, coercing errors to NaT\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d', errors='coerce')\n",
    "    \n",
    "    # Convert 'value' to a numeric type, coercing errors to NaN\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    \n",
    "    # Sort by 'date' to ensure chronological order\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Calculate log of 'value' and the difference in log_price\n",
    "    df['log_price'] = np.log(df['value'])\n",
    "    df['price_change'] = df['log_price'].diff()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to perform AutoARIMA forecasting\n",
    "def forecast_prices(df, cutoff_date):\n",
    "    # Filter the DataFrame based on the cutoff date\n",
    "    df_filtered = df[df['date'] < pd.to_datetime(cutoff_date)]\n",
    "    \n",
    "    # Convert prices to log prices to stabilize variance\n",
    "    df_filtered['log_price'] = np.log(df_filtered['value'])\n",
    "    \n",
    "    # Define the model\n",
    "    model = SARIMAX(df_filtered['log_price'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "    \n",
    "    # Fit the model\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Generate future dates\n",
    "    future_dates = pd.date_range(df_filtered['date'].max() + MonthEnd(1), periods=13, freq='M')\n",
    "    \n",
    "    # Forecast future log prices\n",
    "    forecast_log_prices = results.forecast(steps=13)\n",
    "    \n",
    "    # Convert log prices back to regular prices\n",
    "    forecast_prices = np.exp(forecast_log_prices)\n",
    "    \n",
    "    # Create a DataFrame for the forecasted prices\n",
    "    forecast_df = pd.DataFrame({\n",
    "        'date': future_dates,\n",
    "        'forecast_price': forecast_prices\n",
    "    })\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "# Visualization function\n",
    "def plot_forecast(df, forecast_df):\n",
    "    base = alt.Chart(df).encode(\n",
    "        x='date:T',\n",
    "        y='value:Q'\n",
    "    ).properties(\n",
    "        width=700,\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    line = base.mark_line(color='blue', size=3)\n",
    "    points = base.mark_point(color='red')\n",
    "\n",
    "    forecast_chart = alt.Chart(forecast_df).mark_line(color='green').encode(\n",
    "        x='date:T',\n",
    "        y='forecast_price:Q'\n",
    "    )\n",
    "\n",
    "    return line + points + forecast_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1967d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_gas_price_data('data/raw/PET/PET.txt')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec131a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared = prepare_data(df)\n",
    "df_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6332a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = forecast_prices(df, '2021-01-01')\n",
    "forecast_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca84236",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast(df, forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# import joblib\n",
    "# joblib.dump(model, 'models/oil_price_forecast_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
